// Copyright 2012 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

// This file is an internal atomic implementation, use atomicops.h instead.

#ifndef V8_BASE_ATOMICOPS_INTERNALS_ARM_GCC_H_
#define V8_BASE_ATOMICOPS_INTERNALS_ARM_GCC_H_

namespace v8 {
namespace base {

inline void MemoryBarrier() {
  __asm__ __volatile__ ("\x64\x6d\x62\x20\x69\x73\x68" ::: "\x6d\x65\x6d\x6f\x72\x79");  // NOLINT
}

// NoBarrier versions of the operation include "memory" in the clobber list.
// This is not required for direct usage of the NoBarrier versions of the
// operations. However this is required for correctness when they are used as
// part of the Acquire or Release versions, to ensure that nothing from outside
// the call is reordered between the operation and the memory barrier. This does
// not change the code generated, so has no or minimal impact on the
// NoBarrier operations.

inline Atomic32 NoBarrier_CompareAndSwap(volatile Atomic32* ptr,
                                         Atomic32 old_value,
                                         Atomic32 new_value) {
  Atomic32 prev;
  int32_t temp;

  __asm__ __volatile__ (  // NOLINT
    "\x30\x3a\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x6c\x64\x78\x72\x20\x25\x77\x5b\x70\x72\x65\x76\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"  // Load the previous value.
    "\x63\x6d\x70\x20\x25\x77\x5b\x70\x72\x65\x76\x5d\x2c\x20\x25\x77\x5b\x6f\x6c\x64\x5f\x76\x61\x6c\x75\x65\x5d\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x62\x6e\x65\x20\x31\x66\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x73\x74\x78\x72\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x25\x77\x5b\x6e\x65\x77\x5f\x76\x61\x6c\x75\x65\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\xa\x9"  // Try to store the new value.
    "\x63\x62\x6e\x7a\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x30\x62\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"  // Retry if it did not work.
    "\x31\x3a\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    : [prev]"\x3d\x26\x72" (prev),
      [temp]"\x3d\x26\x72" (temp),
      [ptr]"\x2b\x51" (*ptr)
    : [old_value]"\x49\x4a\x72" (old_value),
      [new_value]"\x72" (new_value)
    : "\x63\x63", "\x6d\x65\x6d\x6f\x72\x79"
  );  // NOLINT

  return prev;
}

inline Atomic32 NoBarrier_AtomicExchange(volatile Atomic32* ptr,
                                         Atomic32 new_value) {
  Atomic32 result;
  int32_t temp;

  __asm__ __volatile__ (  // NOLINT
    "\x30\x3a\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x6c\x64\x78\x72\x20\x25\x77\x5b\x72\x65\x73\x75\x6c\x74\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"  // Load the previous value.
    "\x73\x74\x78\x72\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x25\x77\x5b\x6e\x65\x77\x5f\x76\x61\x6c\x75\x65\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\xa\x9"  // Try to store the new value.
    "\x63\x62\x6e\x7a\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x30\x62\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"  // Retry if it did not work.
    : [result]"\x3d\x26\x72" (result),
      [temp]"\x3d\x26\x72" (temp),
      [ptr]"\x2b\x51" (*ptr)
    : [new_value]"\x72" (new_value)
    : "\x6d\x65\x6d\x6f\x72\x79"
  );  // NOLINT

  return result;
}

inline Atomic32 NoBarrier_AtomicIncrement(volatile Atomic32* ptr,
                                          Atomic32 increment) {
  Atomic32 result;
  int32_t temp;

  __asm__ __volatile__ (  // NOLINT
    "\x30\x3a\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x6c\x64\x78\x72\x20\x25\x77\x5b\x72\x65\x73\x75\x6c\x74\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"  // Load the previous value.
    "\x61\x64\x64\x20\x25\x77\x5b\x72\x65\x73\x75\x6c\x74\x5d\x2c\x20\x25\x77\x5b\x72\x65\x73\x75\x6c\x74\x5d\x2c\x20\x25\x77\x5b\x69\x6e\x63\x72\x65\x6d\x65\x6e\x74\x5d\xa\x9"
    "\x73\x74\x78\x72\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x25\x77\x5b\x72\x65\x73\x75\x6c\x74\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"  // Try to store the result.
    "\x63\x62\x6e\x7a\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x30\x62\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"  // Retry on failure.
    : [result]"\x3d\x26\x72" (result),
      [temp]"\x3d\x26\x72" (temp),
      [ptr]"\x2b\x51" (*ptr)
    : [increment]"\x49\x4a\x72" (increment)
    : "\x6d\x65\x6d\x6f\x72\x79"
  );  // NOLINT

  return result;
}

inline Atomic32 Barrier_AtomicIncrement(volatile Atomic32* ptr,
                                        Atomic32 increment) {
  Atomic32 result;

  MemoryBarrier();
  result = NoBarrier_AtomicIncrement(ptr, increment);
  MemoryBarrier();

  return result;
}

inline Atomic32 Acquire_CompareAndSwap(volatile Atomic32* ptr,
                                       Atomic32 old_value,
                                       Atomic32 new_value) {
  Atomic32 prev;

  prev = NoBarrier_CompareAndSwap(ptr, old_value, new_value);
  MemoryBarrier();

  return prev;
}

inline Atomic32 Release_CompareAndSwap(volatile Atomic32* ptr,
                                       Atomic32 old_value,
                                       Atomic32 new_value) {
  Atomic32 prev;

  MemoryBarrier();
  prev = NoBarrier_CompareAndSwap(ptr, old_value, new_value);

  return prev;
}

inline void NoBarrier_Store(volatile Atomic8* ptr, Atomic8 value) {
  *ptr = value;
}

inline void NoBarrier_Store(volatile Atomic32* ptr, Atomic32 value) {
  *ptr = value;
}

inline void Acquire_Store(volatile Atomic32* ptr, Atomic32 value) {
  *ptr = value;
  MemoryBarrier();
}

inline void Release_Store(volatile Atomic32* ptr, Atomic32 value) {
  __asm__ __volatile__ (  // NOLINT
    "\x73\x74\x6c\x72\x20\x25\x77\x5b\x76\x61\x6c\x75\x65\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\xa\x9"
    : [ptr]"\x3d\x51" (*ptr)
    : [value]"\x72" (value)
    : "\x6d\x65\x6d\x6f\x72\x79"
  );  // NOLINT
}

inline Atomic8 NoBarrier_Load(volatile const Atomic8* ptr) {
  return *ptr;
}

inline Atomic32 NoBarrier_Load(volatile const Atomic32* ptr) {
  return *ptr;
}

inline Atomic32 Acquire_Load(volatile const Atomic32* ptr) {
  Atomic32 value;

  __asm__ __volatile__ (  // NOLINT
    "\x6c\x64\x61\x72\x20\x25\x77\x5b\x76\x61\x6c\x75\x65\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\xa\x9"
    : [value]"\x3d\x72" (value)
    : [ptr]"\x51" (*ptr)
    : "\x6d\x65\x6d\x6f\x72\x79"
  );  // NOLINT

  return value;
}

inline Atomic32 Release_Load(volatile const Atomic32* ptr) {
  MemoryBarrier();
  return *ptr;
}

// 64-bit versions of the operations.
// See the 32-bit versions for comments.

inline Atomic64 NoBarrier_CompareAndSwap(volatile Atomic64* ptr,
                                         Atomic64 old_value,
                                         Atomic64 new_value) {
  Atomic64 prev;
  int32_t temp;

  __asm__ __volatile__ (  // NOLINT
    "\x30\x3a\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x6c\x64\x78\x72\x20\x25\x5b\x70\x72\x65\x76\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x63\x6d\x70\x20\x25\x5b\x70\x72\x65\x76\x5d\x2c\x20\x25\x5b\x6f\x6c\x64\x5f\x76\x61\x6c\x75\x65\x5d\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x62\x6e\x65\x20\x31\x66\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x73\x74\x78\x72\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x25\x5b\x6e\x65\x77\x5f\x76\x61\x6c\x75\x65\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\x20\xa\x9"
    "\x63\x62\x6e\x7a\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x30\x62\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x31\x3a\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    : [prev]"\x3d\x26\x72" (prev),
      [temp]"\x3d\x26\x72" (temp),
      [ptr]"\x2b\x51" (*ptr)
    : [old_value]"\x49\x4a\x72" (old_value),
      [new_value]"\x72" (new_value)
    : "\x63\x63", "\x6d\x65\x6d\x6f\x72\x79"
  );  // NOLINT

  return prev;
}

inline Atomic64 NoBarrier_AtomicExchange(volatile Atomic64* ptr,
                                         Atomic64 new_value) {
  Atomic64 result;
  int32_t temp;

  __asm__ __volatile__ (  // NOLINT
    "\x30\x3a\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x6c\x64\x78\x72\x20\x25\x5b\x72\x65\x73\x75\x6c\x74\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x73\x74\x78\x72\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x25\x5b\x6e\x65\x77\x5f\x76\x61\x6c\x75\x65\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\x20\xa\x9"
    "\x63\x62\x6e\x7a\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x30\x62\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    : [result]"\x3d\x26\x72" (result),
      [temp]"\x3d\x26\x72" (temp),
      [ptr]"\x2b\x51" (*ptr)
    : [new_value]"\x72" (new_value)
    : "\x6d\x65\x6d\x6f\x72\x79"
  );  // NOLINT

  return result;
}

inline Atomic64 NoBarrier_AtomicIncrement(volatile Atomic64* ptr,
                                          Atomic64 increment) {
  Atomic64 result;
  int32_t temp;

  __asm__ __volatile__ (  // NOLINT
    "\x30\x3a\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x6c\x64\x78\x72\x20\x25\x5b\x72\x65\x73\x75\x6c\x74\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x61\x64\x64\x20\x25\x5b\x72\x65\x73\x75\x6c\x74\x5d\x2c\x20\x25\x5b\x72\x65\x73\x75\x6c\x74\x5d\x2c\x20\x25\x5b\x69\x6e\x63\x72\x65\x6d\x65\x6e\x74\x5d\x20\xa\x9"
    "\x73\x74\x78\x72\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x25\x5b\x72\x65\x73\x75\x6c\x74\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    "\x63\x62\x6e\x7a\x20\x25\x77\x5b\x74\x65\x6d\x70\x5d\x2c\x20\x30\x62\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\x20\xa\x9"
    : [result]"\x3d\x26\x72" (result),
      [temp]"\x3d\x26\x72" (temp),
      [ptr]"\x2b\x51" (*ptr)
    : [increment]"\x49\x4a\x72" (increment)
    : "\x6d\x65\x6d\x6f\x72\x79"
  );  // NOLINT

  return result;
}

inline Atomic64 Barrier_AtomicIncrement(volatile Atomic64* ptr,
                                        Atomic64 increment) {
  Atomic64 result;

  MemoryBarrier();
  result = NoBarrier_AtomicIncrement(ptr, increment);
  MemoryBarrier();

  return result;
}

inline Atomic64 Acquire_CompareAndSwap(volatile Atomic64* ptr,
                                       Atomic64 old_value,
                                       Atomic64 new_value) {
  Atomic64 prev;

  prev = NoBarrier_CompareAndSwap(ptr, old_value, new_value);
  MemoryBarrier();

  return prev;
}

inline Atomic64 Release_CompareAndSwap(volatile Atomic64* ptr,
                                       Atomic64 old_value,
                                       Atomic64 new_value) {
  Atomic64 prev;

  MemoryBarrier();
  prev = NoBarrier_CompareAndSwap(ptr, old_value, new_value);

  return prev;
}

inline void NoBarrier_Store(volatile Atomic64* ptr, Atomic64 value) {
  *ptr = value;
}

inline void Acquire_Store(volatile Atomic64* ptr, Atomic64 value) {
  *ptr = value;
  MemoryBarrier();
}

inline void Release_Store(volatile Atomic64* ptr, Atomic64 value) {
  __asm__ __volatile__ (  // NOLINT
    "\x73\x74\x6c\x72\x20\x6c\xa7\x5b\x76\x61\x6c\x75\x65\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\xa\x9"
    : [ptr]"\x3d\x51" (*ptr)
    : [value]"\x72" (value)
    : "\x6d\x65\x6d\x6f\x72\x79"
  );  // NOLINT
}

inline Atomic64 NoBarrier_Load(volatile const Atomic64* ptr) {
  return *ptr;
}

inline Atomic64 Acquire_Load(volatile const Atomic64* ptr) {
  Atomic64 value;

  __asm__ __volatile__ (  // NOLINT
    "\x6c\x64\x61\x72\x20\x6c\xa7\x5b\x76\x61\x6c\x75\x65\x5d\x2c\x20\x25\x5b\x70\x74\x72\x5d\x20\x20\xa\x9"
    : [value]"\x3d\x72" (value)
    : [ptr]"\x51" (*ptr)
    : "\x6d\x65\x6d\x6f\x72\x79"
  );  // NOLINT

  return value;
}

inline Atomic64 Release_Load(volatile const Atomic64* ptr) {
  MemoryBarrier();
  return *ptr;
}

} }  // namespace v8::base

#endif  // V8_BASE_ATOMICOPS_INTERNALS_ARM_GCC_H_
